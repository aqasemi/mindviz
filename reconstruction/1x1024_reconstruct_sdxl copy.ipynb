{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.34.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_embeds = np.load(\"/home/qasemiaa/mjo/upb/exp/eeg_intra-subject_ubp_EEGProjectLayer_ViT-H-14/sub-08_seed29/test_embeddings.npz\", allow_pickle=True)\n",
    "test_eeg_embeds = torch.tensor(test_embeds['eeg_embeddings'], dtype=torch.float32).to(device)\n",
    "test_image_embeds = torch.tensor(test_embeds['img_embeddings'], dtype=torch.float32).to(device)\n",
    "\n",
    "# train_embeds = np.load(\"/home/qasemiaa/mjo/upb/exp/eeg_intra-subject_ubp_EEGProjectLayer_ViT-H-14/sub-08_seed29/train_embeddings.npz\", allow_pickle=True)\n",
    "# train_eeg_embeds = torch.tensor(train_embeds['eeg_embeddings'], dtype=torch.float32).to(device)\n",
    "# train_image_embeds = torch.tensor(train_embeds['img_embeddings'], dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "ret_test_embeds = np.load(\"/home/qasemiaa/mjo/upb/exp/eeg_intra-subject_ubp_EEGProjectLayer_RN50/sub-08_seed0/test_embeddings.npz\", allow_pickle=True)\n",
    "ret_test_img_embeds = torch.tensor(ret_test_embeds['img_embeddings'], dtype=torch.float32)\n",
    "ret_test_eeg_embeds = torch.tensor(ret_test_embeds['eeg_embeddings'], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k(eeg_z, img_z, k=5):\n",
    "    eeg_z = eeg_z/eeg_z.norm(dim=-1, keepdim=True)\n",
    "    similarity = (eeg_z @ img_z.T)\n",
    "    top_kvalues, top_k_indices = similarity.topk(k, dim=-1)\n",
    "    return top_k_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qasemiaa/mjo/jkk/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/qasemiaa/mjo/jkk/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/qasemiaa/mjo/jkk/.venv/lib/python3.12/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from EEG_Image_decode.Generation.diffusion_prior import *\n",
    "# from EEG_Image_decode.Generation.custom_pipeline_low_level import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.color import rgb2gray\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def get_ssim(rec, img):\n",
    "    recon_gray = rgb2gray(rec.resize((500, 500)))\n",
    "    img_gray = rgb2gray(img.resize((500, 500)))\n",
    "    return ssim(recon_gray, img_gray, multichannel=True, gaussian_weights=True, sigma=1.5, use_sample_covariance=False, data_range=1.0)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False # doesn't work for train; gotta use 'indicies' values to order the images\n",
    "classes = None\n",
    "pictures = None\n",
    "\n",
    "def load_data():\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    texts = []\n",
    "    images = []\n",
    "    \n",
    "    if train:\n",
    "        text_directory = \"/ibex/user/qasemiaa/datasets/things_eeg/image_set/training_images\"\n",
    "    else:\n",
    "        text_directory = \"/ibex/user/qasemiaa/datasets/things_eeg/image_set/test_images\"\n",
    "    dirnames = [d for d in os.listdir(text_directory) if os.path.isdir(os.path.join(text_directory, d))]\n",
    "    dirnames.sort()\n",
    "    \n",
    "    if classes is not None:\n",
    "        dirnames = [dirnames[i] for i in classes]\n",
    "\n",
    "    for dir in dirnames:\n",
    "\n",
    "        try:\n",
    "            idx = dir.index('_')\n",
    "            description = dir[idx+1:]\n",
    "        except ValueError:\n",
    "            print(f\"Skipped: {dir} due to no '_' found.\")\n",
    "            continue\n",
    "            \n",
    "        new_description = f\"{description}\"\n",
    "        texts.append(new_description)\n",
    "\n",
    "    if train:\n",
    "        img_directory = \"/ibex/user/qasemiaa/datasets/things_eeg/image_set/training_images\"\n",
    "    else:\n",
    "        img_directory =\"/ibex/user/qasemiaa/datasets/things_eeg/image_set/test_images\"\n",
    "    \n",
    "    all_folders = [d for d in os.listdir(img_directory) if os.path.isdir(os.path.join(img_directory, d))]\n",
    "    all_folders.sort()\n",
    "\n",
    "    if classes is not None and pictures is not None:\n",
    "        images = []\n",
    "        for i in range(len(classes)):\n",
    "            class_idx = classes[i]\n",
    "            pic_idx = pictures[i]\n",
    "            if class_idx < len(all_folders):\n",
    "                folder = all_folders[class_idx]\n",
    "                folder_path = os.path.join(img_directory, folder)\n",
    "                all_images = [img for img in os.listdir(folder_path) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "                all_images.sort()\n",
    "                if pic_idx < len(all_images):\n",
    "                    images.append(os.path.join(folder_path, all_images[pic_idx]))\n",
    "    elif classes is not None and pictures is None:\n",
    "        images = []\n",
    "        for i in range(len(classes)):\n",
    "            class_idx = classes[i]\n",
    "            if class_idx < len(all_folders):\n",
    "                folder = all_folders[class_idx]\n",
    "                folder_path = os.path.join(img_directory, folder)\n",
    "                all_images = [img for img in os.listdir(folder_path) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "                all_images.sort()\n",
    "                images.extend(os.path.join(folder_path, img) for img in all_images)\n",
    "    elif classes is None:\n",
    "        images = []\n",
    "        for folder in all_folders:\n",
    "            folder_path = os.path.join(img_directory, folder)\n",
    "            all_images = [img for img in os.listdir(folder_path) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            all_images.sort()  \n",
    "            images.extend(os.path.join(folder_path, img) for img in all_images)\n",
    "    else:\n",
    "\n",
    "        print(\"Error\")\n",
    "    return texts, images\n",
    "texts, images = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qasemiaa/mjo/jkk/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aeb8c4e72594ef7aaba6ec3310c909f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/qasemiaa/mjo/jkk/.venv/lib/python3.12/site-packages/diffusers/utils/outputs.py:63: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "generator = Generator4Embeds(num_inference_steps=4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Sample 0 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71144a743dc54105ac0e392a8d35180d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Assuming generator.generate returns a PIL Image\n",
    "sub = \"08\"\n",
    "directory = f\"generated_imgs/{sub}\"\n",
    "for k in range(40): \n",
    "    print(f\"--- Processing Sample {k} ---\")\n",
    "    num_generated = 2\n",
    "    \n",
    "    eeg_embeds = test_eeg_embeds[k:k+1]\n",
    "    top_g = top_k(ret_test_eeg_embeds[k:k+1], ret_test_img_embeds, 2)[0].tolist()\n",
    "    fig, axes = plt.subplots(1, num_generated + 1, figsize=(15, 5))\n",
    "    fig.suptitle(f'Results for Sample {k}', fontsize=16)\n",
    "\n",
    "\n",
    "    for j in range(num_generated):\n",
    "        image_class_name = ' '.join(Path(images[top_g[j]]).name.split(\"_\")[:-1])\n",
    "        image = generator.generate(eeg_embeds.unsqueeze(0).to(dtype=torch.float16), text_prompt=image_class_name)\n",
    "\n",
    "        image_dir = f'{directory}/image_{k}'\n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "        image.save(f'{image_dir}/{j}.png')\n",
    "        \n",
    "        axes[j].imshow(image)\n",
    "        axes[j].set_title(f'Image {j+1} ') # Class: {image_class_name}')\n",
    "        axes[j].axis('off') \n",
    "\n",
    "    # Save the ground truth image in the same folder\n",
    "    gt_path = images[k]\n",
    "    gt_image = Image.open(gt_path).convert(\"RGB\") \n",
    "    gt_save_path = f'{directory}/image_{k}/gt.png'\n",
    "    gt_image.save(gt_save_path)\n",
    "    \n",
    "    # Plot the ground truth image in the last subplot\n",
    "    axes[num_generated].imshow(gt_image)\n",
    "    axes[num_generated].set_title('Ground Truth')\n",
    "    axes[num_generated].axis('off')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96]) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
